{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr7FO0jbdLNe",
    "outputId": "db04a8c2-59c4-4aec-d162-e7714208dc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OurDQUtbWQacvT32HMqFL7vIUrSMllOp\n",
      "To: /content/preprocessed_data.csv\n",
      "\r",
      "  0% 0.00/300k [00:00<?, ?B/s]\r",
      "100% 300k/300k [00:00<00:00, 9.72MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1OurDQUtbWQacvT32HMqFL7vIUrSMllOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "L8fjyOdpdLRC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu95qXCh0b04",
    "outputId": "e623222f-1033-4b7b-ea8e-8c6c87d336d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "ZIhGIeQK0bv-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "To use the Kaggle API, sign up for a Kaggle account at https://www.kaggle.com. \n",
    "Then go to the 'Account' tab of your user profile (https://www.kaggle.com/<username>/account) and select 'Create API Token'. \n",
    "This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "Upload that file to google colab/google cloud platform \n",
    "\"\"\"\n",
    "api_token = {\"username\":\"manojkumar83000\",\"key\":\"a6c354dd1bc5460d07ffb4844b923064\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5Nq7UYB0b4U",
    "outputId": "c388165a-1a21-4c05-a7c2-19c25f523b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "fasttext-crawl-300d-2m.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d yekenot/fasttext-crawl-300d-2m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1YtmWMr0b_K"
   },
   "outputs": [],
   "source": [
    "!7z e fasttext-crawl-300d-2m.zip -o/content -r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8kDYO560cCl"
   },
   "outputs": [],
   "source": [
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def fasttextModel(gloveFile):\n",
    "    print (\"Loading Fasttext Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}#for storing word and the corresponding embedding vector for that word\n",
    "    for line in f:\n",
    "        splitLine = line.split()#splitting the line and storing it in a list\n",
    "        word = splitLine[0]#getting the first element and storing it in word\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])#obtaining corresponding vector for that word\n",
    "        model[word] = embedding#storing word as key and embedding vector for that word as value\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = fasttextModel('/content/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "vwVkeKqQdLTt"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "w24_JGmDdLWL",
    "outputId": "79f82c58-a89a-4682-ddd9-3bc198efff3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?\\n</td>\n",
       "      <td>Do you want me to reserve seat for you or not?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm thai. what do u do?\\n</td>\n",
       "      <td>I'm Thai. What do you do?\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                             target\n",
       "0           0  ...   Do you want me to reserve seat for you or not?\\n\n",
       "1           1  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2           2  ...  They become more expensive already. Mine is li...\n",
       "3           3  ...                        I'm Thai. What do you do?\\n\n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "cSbtR_HldLZo"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  x=x[:-1]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "FOLd0rnFdLjx"
   },
   "outputs": [],
   "source": [
    "df['source']=df['source'].apply(preprocess)\n",
    "df['target']=df['target'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "I2MMzLqQdLms",
    "outputId": "11be80ea-2e24-49e2-a79b-a8eb9f89b046"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>I'm Thai. What do you do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source                                             target\n",
       "0                    U wan me to \"chop\" seat 4 u nt?     Do you want me to reserve seat for you or not?\n",
       "1  Yup. U reaching. We order some durian pastry a...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?                          I'm Thai. What do you do?\n",
       "4  Hi! How did your week go? Haven heard from you...  Hi! How did your week go? Haven't heard from y..."
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['source','target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLbB1qx6oja7",
    "outputId": "ffc48876-f391-4992-db41-26a7ca516e67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 163,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "Kn8IMU4LoZSW"
   },
   "outputs": [],
   "source": [
    "def length(text):#for calculating the length of the sentence\n",
    "    return len(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "MlN7BR2tdLqK"
   },
   "outputs": [],
   "source": [
    "df=df[df['source'].apply(length)<170]\n",
    "df=df[df['target'].apply(length)<200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJiyjBaZfbqD",
    "outputId": "b837f702-2629-491b-9402-935b4abef530"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 2)"
      ]
     },
     "execution_count": 171,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "sFhX3qniszPG",
    "outputId": "bac4aa7a-b37e-40e3-b448-7e22e8f8d7fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>target_in</th>\n",
       "      <th>target_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?</td>\n",
       "      <td>&lt;start&gt; Do you want me to reserve seat for you...</td>\n",
       "      <td>Do you want me to reserve seat for you or not?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "      <td>&lt;start&gt; Yeap. You reaching? We ordered some Du...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "      <td>&lt;start&gt; They become more expensive already. Mi...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>I'm Thai. What do you do?</td>\n",
       "      <td>&lt;start&gt; I'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "      <td>&lt;start&gt; Hi! How did your week go? Haven't hear...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  ...                                         target_out\n",
       "0                    U wan me to \"chop\" seat 4 u nt?  ...  Do you want me to reserve seat for you or not?...\n",
       "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?  ...                    I'm Thai. What do you do? <end>\n",
       "4  Hi! How did your week go? Haven heard from you...  ...  Hi! How did your week go? Haven't heard from y...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target_in'] = '<start> ' + df['target'].astype(str)\n",
    "df['target_out'] = df['target'].astype(str) + ' <end>'\n",
    "# only for the first sentance add a toke <end> so that we will have <end> in tokenizer\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "boQSuOdzszZf"
   },
   "outputs": [],
   "source": [
    "df=df.drop('target',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "2IZQxQheF1Ew",
    "outputId": "65bc4638-5371-4339-aeb4-44077bae949b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target_in</th>\n",
       "      <th>target_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>&lt;start&gt; Do you want me to reserve seat for you...</td>\n",
       "      <td>Do you want me to reserve seat for you or not?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>&lt;start&gt; Yeap. You reaching? We ordered some Du...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>&lt;start&gt; They become more expensive already. Mi...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>&lt;start&gt; I'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  ...                                         target_out\n",
       "0                    U wan me to \"chop\" seat 4 u nt?  ...  Do you want me to reserve seat for you or not?...\n",
       "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?  ...                    I'm Thai. What do you do? <end>\n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "zY_3AB8NF_zb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(df, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9dvlVbGGNzp",
    "outputId": "0fd24266-791d-4b89-aab0-5d283dc3bb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, validation.shape)\n",
    "# for one sentence we will be adding <end> token so that the tokanizer learns the word <end>\n",
    "# with this we can use only one tokenizer for both encoder output and decoder output\n",
    "train.iloc[0]['target_in']= str(train.iloc[0]['target_in'])+' <end>'\n",
    "train.iloc[0]['target_out']= str(train.iloc[0]['target_out'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "r7unX08JGY7i"
   },
   "outputs": [],
   "source": [
    "tknizer_source = Tokenizer()\n",
    "tknizer_source.fit_on_texts(train['source'].values)\n",
    "tknizer_target = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_target.fit_on_texts(train['target_in'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wdrwjwt3G1U_",
    "outputId": "11e9297f-835e-43b4-ba21-31ca1e3133c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3038\n",
      "3706\n"
     ]
    }
   ],
   "source": [
    "vocab_size_target=len(tknizer_target.word_index.keys())\n",
    "print(vocab_size_target)\n",
    "vocab_size_source=len(tknizer_source.word_index.keys())\n",
    "print(vocab_size_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "yJrXDwFD0NIY"
   },
   "outputs": [],
   "source": [
    "decoder_embedding_matrix = np.zeros((vocab_size_target+1, 300))\n",
    "for word, i in tknizer_target.word_index.items():\n",
    "    embedding_vector = model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        decoder_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "B_r9uT620Nss"
   },
   "outputs": [],
   "source": [
    "encoder_embedding_matrix = np.zeros((vocab_size_source+1, 300))\n",
    "for word, i in tknizer_source.word_index.items():\n",
    "    embedding_vector = model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        encoder_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDFjkMRd1XGg",
    "outputId": "3044d7a9-8889-490e-e543-c1ab77256ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3039, 300)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qat7pkQ1XPp",
    "outputId": "1930b61c-4c3f-4a72-ba0f-829f9e5d08ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3707, 300)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5VU3LonHHh-",
    "outputId": "68281e9e-6923-4a4d-e0f2-a69ab7d96656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1445)"
      ]
     },
     "execution_count": 187,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknizer_target.word_index['<start>'], tknizer_target.word_index['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "HMPfMXTaHQ2T"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        super().__init__()\n",
    "        self.vocab_size = inp_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_length = input_length\n",
    "        self.lstm_size= lstm_size\n",
    "        self.lstm_output=0\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
    "                           mask_zero=True,weights=[encoder_embedding_matrix],name=\"embedding_layer_encoder\",trainable=False)\n",
    "        self.lstm = tf.keras.layers.LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "\n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer oupulstm_state_h,lstm_state_ct to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      \n",
    "      input_embedd                           = self.embedding(input_sequence)\n",
    "      lstm_state_h,lstm_state_c= states[0],states[1]\n",
    "      self.lstm_output,lstm_state_h,lstm_state_c=self.lstm(input_embedd)\n",
    "      return self.lstm_output,lstm_state_h,lstm_state_c\n",
    "\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      return [tf.zeros((batch_size,self.lstm_size)),tf.zeros((batch_size,self.lstm_size))]\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "u1ww6VsdHwAA"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super().__init__()\n",
    "    self.scoring_function=scoring_function\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      pass\n",
    "    if self.scoring_function == 'general':\n",
    "      # Intialize variables needed for General score function here\n",
    "      self.weight=tf.keras.layers.Dense(att_units)\n",
    "    elif self.scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "      self.weight1=tf.keras.layers.Dense(att_units)\n",
    "      self.weight2=tf.keras.layers.Dense(att_units)\n",
    "      self.v=tf.keras.layers.Dense(1)\n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=2)\n",
    "        value=tf.matmul(encoder_output,decoder_hidden_state)\n",
    "    elif self.scoring_function == 'general':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=2)\n",
    "        value=tf.matmul(self.weight(encoder_output),decoder_hidden_state)\n",
    "    elif self.scoring_function == 'concat':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=1)\n",
    "        value=self.v(tf.nn.tanh(self.weight1(decoder_hidden_state)+self.weight2(encoder_output)))\n",
    "    attention_weights=tf.nn.softmax(value,axis=1)\n",
    "    context_vector=attention_weights*encoder_output\n",
    "    return tf.reduce_sum(context_vector,axis=1),attention_weights\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "DhNfgUq-H0SF"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super().__init__()\n",
    "      self.tar_vocab_size = tar_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length \n",
    "      self.dec_units = dec_units\n",
    "      self.score_fun = score_fun\n",
    "      self.att_units = att_units\n",
    "      # we are using embedding_matrix and not training the embedding layer\n",
    "      self.embedding = tf.keras.layers.Embedding(input_dim=self.tar_vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True,weights=[decoder_embedding_matrix],name=\"embedding_layer_decoder\", trainable=False)\n",
    "      self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True)\n",
    "      self.dense = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "      self.attention = Attention(self.score_fun,self.att_units)\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    output = self.embedding(input_to_decoder)\n",
    "    context_vector,attention_weights = self.attention(state_h,encoder_output)\n",
    "    context_vector1 = tf.expand_dims(context_vector,1)\n",
    "    concat = tf.concat([output,context_vector1],axis=-1)\n",
    "    decoder_output,state_h,state_c = self.lstm(concat,initial_state=[state_h,state_c])\n",
    "    final_output = self.dense(decoder_output)\n",
    "    final_output = tf.reshape(final_output,(-1,final_output.shape[2]))\n",
    "    return final_output,state_h,state_c,attention_weights,context_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "97Rryo9n8VRH"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super(Decoder,self).__init__()\n",
    "      self.vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units=dec_units\n",
    "      self.att_units=att_units\n",
    "      self.score_fun=score_fun\n",
    "      self.onestepdecoder=One_Step_Decoder(self.vocab_size,self.embedding_dim,self.input_length,self.dec_units ,self.score_fun ,self.att_units)\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        all_outputs=tf.TensorArray(tf.float32,size=input_to_decoder.shape[1])\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        #Iterate till the length of the decoder input\n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output,state_h,state_c,attention_weights,context_vector=self.onestepdecoder(input_to_decoder[:,timestep:timestep+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
    "\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs=all_outputs.write(timestep,output)\n",
    "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        # Return the tensor array\n",
    "        return all_outputs\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "3Up5Qc-O8ZNa"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,encoder_inputs_length,decoder_inputs_length, output_vocab_size,batch_size,score_fun):\n",
    "    #Intialize objects from encoder decoder\n",
    "    super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "    self.batch_size=batch_size\n",
    "    self.encoder = Encoder(vocab_size_source+1,300,100,encoder_inputs_length)\n",
    "    self.decoder = Decoder(vocab_size_target+1,300,decoder_inputs_length,100,score_fun,100)\n",
    "    \n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    input,output = data[0], data[1]\n",
    "    initial_state=self.encoder.initialize_states(self.batch_size)\n",
    "    encoder_output, encoder_h, encoder_c = self.encoder(input,initial_state)\n",
    "    decoder_output= self.decoder(output, encoder_output, encoder_h, encoder_c)\n",
    "    return decoder_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "2UaWKKXL8cNb"
   },
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
    "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
    "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
    "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
    "    during preprocessing to make equal length for all the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "preo2243H7Qh"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, tknizer_source, tknizer_target, source_len,target_len):\n",
    "        self.encoder_inps = df['source'].values\n",
    "        self.decoder_inps = df['target_in'].values\n",
    "        self.decoder_outs = df['target_out'].values\n",
    "        self.tknizer_target = tknizer_target\n",
    "        self.tknizer_source = tknizer_source\n",
    "        self.source_len = source_len\n",
    "        self.target_len = target_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_source.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_target.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_target.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.source_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.target_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.target_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hahhnu0PIgjo",
    "outputId": "6ec1934d-c772-475c-f2e1-64b673748222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 39) (512, 43) (512, 43)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_source, tknizer_target,39,43)\n",
    "test_dataset  = Dataset(validation, tknizer_source, tknizer_target,39,43)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=20)\n",
    "\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "S3mtq-6TI62z"
   },
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "XToH15GbJCHI"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIx_zCBDJEKu",
    "outputId": "389d3123-9e09-4d26-80a1-2d53c60a5e74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 2s 727ms/step - loss: 2.6743 - val_loss: 2.8001\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 3.1049 - val_loss: 2.6459\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 2s 701ms/step - loss: 2.8063 - val_loss: 2.4101\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 2s 719ms/step - loss: 2.6555 - val_loss: 2.3383\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 2.5396 - val_loss: 2.2771\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 2s 777ms/step - loss: 2.4468 - val_loss: 2.1897\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.3849 - val_loss: 2.0806\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.2844 - val_loss: 2.0817\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 2s 744ms/step - loss: 2.2685 - val_loss: 2.0791\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 2s 716ms/step - loss: 2.2113 - val_loss: 2.0348\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 2.2045 - val_loss: 2.0618\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 2s 717ms/step - loss: 2.1675 - val_loss: 2.0392\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 2s 710ms/step - loss: 2.1733 - val_loss: 2.0068\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 2.1584 - val_loss: 2.0042\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 2s 763ms/step - loss: 2.1371 - val_loss: 1.9930\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 2.1129 - val_loss: 2.0101\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 2s 710ms/step - loss: 2.1117 - val_loss: 2.0135\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 2s 703ms/step - loss: 2.1219 - val_loss: 1.9791\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 2s 719ms/step - loss: 2.1001 - val_loss: 1.9775\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.1127 - val_loss: 1.9801\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 2s 721ms/step - loss: 2.1343 - val_loss: 2.0149\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 2s 707ms/step - loss: 2.1217 - val_loss: 1.9798\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 2s 720ms/step - loss: 2.1119 - val_loss: 1.9931\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 2s 726ms/step - loss: 2.1105 - val_loss: 1.9835\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 2s 706ms/step - loss: 2.1120 - val_loss: 1.9812\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 2s 707ms/step - loss: 2.1272 - val_loss: 2.0150\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.1338 - val_loss: 1.9878\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.1208 - val_loss: 2.0014\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.1082 - val_loss: 1.9929\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 2s 702ms/step - loss: 2.1146 - val_loss: 1.9973\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 2s 703ms/step - loss: 2.1049 - val_loss: 2.0026\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 2s 703ms/step - loss: 2.1250 - val_loss: 1.9729\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.1311 - val_loss: 1.9957\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 2s 706ms/step - loss: 2.1171 - val_loss: 1.9856\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 2s 705ms/step - loss: 2.1256 - val_loss: 2.0031\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 2s 706ms/step - loss: 2.1054 - val_loss: 1.9840\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 2.0978 - val_loss: 1.9751\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 2s 772ms/step - loss: 2.0975 - val_loss: 1.9841\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 2s 731ms/step - loss: 2.0863 - val_loss: 1.9830\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 2s 745ms/step - loss: 2.1015 - val_loss: 1.9927\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.1103 - val_loss: 1.9815\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 2s 716ms/step - loss: 2.1009 - val_loss: 1.9851\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 2s 707ms/step - loss: 2.0853 - val_loss: 1.9844\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 2.1094 - val_loss: 1.9984\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 2s 703ms/step - loss: 2.1050 - val_loss: 1.9882\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 2s 702ms/step - loss: 2.1141 - val_loss: 1.9824\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.1094 - val_loss: 2.0025\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.1121 - val_loss: 1.9829\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 2s 791ms/step - loss: 2.0847 - val_loss: 1.9925\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 2s 711ms/step - loss: 2.0908 - val_loss: 1.9837\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 2.0827 - val_loss: 1.9892\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 2s 706ms/step - loss: 2.0754 - val_loss: 1.9712\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 2s 721ms/step - loss: 2.1039 - val_loss: 1.9608\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.0708 - val_loss: 2.0018\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.1111 - val_loss: 2.0088\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.0874 - val_loss: 1.9793\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 2s 705ms/step - loss: 2.1122 - val_loss: 1.9807\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.1115 - val_loss: 2.0028\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 2s 706ms/step - loss: 2.1093 - val_loss: 1.9878\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 2s 785ms/step - loss: 2.1056 - val_loss: 1.9987\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 2s 705ms/step - loss: 2.1165 - val_loss: 1.9693\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 2s 716ms/step - loss: 2.1020 - val_loss: 1.9992\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.0814 - val_loss: 1.9966\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 2s 715ms/step - loss: 2.1038 - val_loss: 1.9744\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 2s 702ms/step - loss: 2.1183 - val_loss: 2.0225\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 2s 717ms/step - loss: 2.1104 - val_loss: 1.9974\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 2.1058 - val_loss: 1.9886\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 2s 719ms/step - loss: 2.1088 - val_loss: 2.0191\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 2.0988 - val_loss: 1.9854\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 2s 782ms/step - loss: 2.1329 - val_loss: 2.0273\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 2s 743ms/step - loss: 2.1320 - val_loss: 1.9902\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 2s 722ms/step - loss: 2.0959 - val_loss: 2.0073\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 2.0861 - val_loss: 2.0226\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 2s 721ms/step - loss: 2.0886 - val_loss: 1.9666\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 2s 734ms/step - loss: 2.0987 - val_loss: 1.9815\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.1030 - val_loss: 1.9998\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.1003 - val_loss: 1.9984\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 2s 709ms/step - loss: 2.0706 - val_loss: 1.9761\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 2s 719ms/step - loss: 2.1079 - val_loss: 1.9823\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 2.0910 - val_loss: 1.9905\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 2s 790ms/step - loss: 2.0991 - val_loss: 2.0074\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 2s 725ms/step - loss: 2.0768 - val_loss: 1.9801\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 2s 711ms/step - loss: 2.1060 - val_loss: 1.9789\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 2s 710ms/step - loss: 2.0763 - val_loss: 1.9917\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 2.0920 - val_loss: 1.9878\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 2s 718ms/step - loss: 2.0883 - val_loss: 2.0033\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 2s 702ms/step - loss: 2.0801 - val_loss: 1.9853\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 2s 713ms/step - loss: 2.0882 - val_loss: 1.9630\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 2s 731ms/step - loss: 2.0831 - val_loss: 1.9978\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 2s 724ms/step - loss: 2.0773 - val_loss: 1.9906\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 2.0675 - val_loss: 1.9766\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 2s 704ms/step - loss: 2.0673 - val_loss: 1.9759\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 2s 767ms/step - loss: 2.0663 - val_loss: 1.9672\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 2s 711ms/step - loss: 2.0774 - val_loss: 1.9735\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 2s 730ms/step - loss: 2.0841 - val_loss: 1.9904\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 2.0921 - val_loss: 1.9782\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 2s 705ms/step - loss: 2.0842 - val_loss: 1.9968\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 2s 711ms/step - loss: 2.1122 - val_loss: 1.9860\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 2s 738ms/step - loss: 2.1077 - val_loss: 1.9917\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 2.0866 - val_loss: 2.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff74b6e5c50>"
      ]
     },
     "execution_count": 205,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an object of encoder_decoder Model class, \n",
    "# Compile the model and fit the model\n",
    "# Implement teacher forcing while training your model. You can do it two ways.\n",
    "# Prepare your data, encoder_input,decoder_input and decoder_output\n",
    "# if decoder input is \n",
    "# <start> Hi how are you\n",
    "# decoder output should be\n",
    "# Hi How are you <end>\n",
    "# i.e when you have send <start>-- decoder predicted Hi, 'Hi' decoder predicted 'How' .. e.t.c\n",
    "\n",
    "# or\n",
    " \n",
    "# model.fit([train_ita,train_eng],train_eng[:,1:]..)\n",
    "# Note: If you follow this approach some grader functions might return false and this is fine.\n",
    "model  = encoder_decoder(encoder_inputs_length=39,decoder_inputs_length=43,output_vocab_size=vocab_size_target,batch_size=512,score_fun=\"dot\")\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "model.compile(optimizer=optimizer,loss=loss_function)\n",
    "train_steps=train.shape[0]//512\n",
    "valid_steps=validation.shape[0]//20\n",
    "model.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=100, validation_data=test_dataloader, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "AK-rVmwTQ1WK"
   },
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "units=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Iufu14J0JVp7"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "  E. Call plot_attention(#params)\n",
    "  F. Return the predicted sentence\n",
    "  '''\n",
    "  initial_state_enc=[np.zeros((batch_size,units)),np.zeros((batch_size,units))]\n",
    "  inp_seq = tknizer_source.texts_to_sequences([input_sentence])\n",
    "  inp_seq = pad_sequences(inp_seq,padding='post',maxlen=39)\n",
    "\n",
    "  en_outputs,state_h , state_c = model.layers[0](tf.constant(inp_seq),initial_state_enc)\n",
    "  cur_vec = tf.constant([[tknizer_target.word_index['<start>']]])\n",
    "  pred = []\n",
    "  #Here 43 is the max_length of the sequence\n",
    "  for i in range(43):\n",
    "    output,state_h,state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(cur_vec,en_outputs,state_h,state_c)\n",
    "    cur_vec = np.reshape(np.argmax(output), (1, 1))\n",
    "    pred.append(tknizer_target.index_word[cur_vec[0][0]])\n",
    "    if(pred[-1]=='<end>'):\n",
    "      break\n",
    "    translated_sentence = ' '.join(pred)\n",
    "  return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axGMlispS2ex",
    "outputId": "468931b2-2a31-4182-d731-15d1e8d204ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1279    <start> At some coffee shop behind a building....\n",
       "1372    <start> Shuhui in Ang Mo Kio, she asks if want...\n",
       "796     <start> I'm not working. What time is Junmei a...\n",
       "1230    <start> Told you to go to Bugis already. Very ...\n",
       "1946    <start> Oh, that guy who is much taller than m...\n",
       "661     <start> Joan never replied me. Call her but sh...\n",
       "1546    <start> Hi, never worry about the truth becaus...\n",
       "129                                <start> Saturday. Can?\n",
       "1668                   <start> Huh? How come, too taxing?\n",
       "325     <start> I'm going for lecture later. So pick m...\n",
       "1441                            <start> Just left office.\n",
       "1195    <start> Your chauffeur? Hahaha, who is it? Fro...\n",
       "460     <start> You looking for June? Came back must P...\n",
       "1804    <start> That pest's father's handphone. Then y...\n",
       "771     <start> So sad. I bought the opera bar without...\n",
       "1109    <start> Yay! I am taking ST and LSM this term....\n",
       "534     <start> HKY, I remember I have to give you $30...\n",
       "929        <start> Ah, I'm in exam period. Ah, I'm dying.\n",
       "134           <start> Wu Jian Dao got sneaks? I anything.\n",
       "1236    <start> Yes. By the way, I'll be buying the pr...\n",
       "Name: target_in, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation['target_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nknpSF7BRs1b",
    "outputId": "058b536b-c0b5-4870-d021-d58699ffd65f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted output is:  hey i was accurate club a check mrt movie question is dividend\n",
      "The predicted output is:  i see you know ok then i am on thursday mummy stuff which 225\n",
      "The predicted output is:  i am accurate club getting mrt station\n",
      "The predicted output is:  hey i need to go\n",
      "The predicted output is:  hey i think they kill me to go project you know ok i know ok i know ok\n",
      "The predicted output is:  i am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not\n",
      "The predicted output is:  i was nus 30pm\n",
      "The predicted output is:  then i am accurate put you get here\n",
      "The predicted output is:  what you know ok\n",
      "The predicted output is:  i meet tomorrow\n",
      "The predicted output is:  we are you know ok i need to go to go to go time off\n",
      "The predicted output is:  hey i was now\n",
      "The predicted output is:  hey i was happy number\n",
      "The predicted output is:  i am on thursday on on thursday yet see you have check\n",
      "The predicted output is:  weight work not having put done you know ok\n",
      "The predicted output is:  you are you know ok i need to go\n",
      "The predicted output is:  hey i was a exercise don't worry\n",
      "The predicted output is:  hey you are you know how have you know how have you know how have you know mrt whole days either mrt outing because it got study hard in 25 you come week tell you know ok i need to go to go\n",
      "The predicted output is:  hey you want to go\n",
      "The predicted output is:  i got first to go for a bit 40 happy interview you colour mrt station\n"
     ]
    }
   ],
   "source": [
    "for i in validation['source']:\n",
    "  predicted=predict(i)\n",
    "  print(\"The predicted output is: \",predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnpQs8_gLAQr",
    "outputId": "f61ffe9e-1d6f-4ae7-a5a6-a61a2d064355"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "import nltk.translate.bleu_score as bleu\n",
    "bleu_scores_lst=[]\n",
    "for i in validation[:]['source']:\n",
    "  reference = [i.split(),] # the original\n",
    "  predicted=predict([i])\n",
    "  translation = predicted.split()\n",
    "  values=bleu.sentence_bleu(reference, translation)\n",
    "  bleu_scores_lst.append(values)\n",
    "\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgU2eYtdQpUF",
    "outputId": "b04aa79a-fb9e-4386-bfdd-9e26cc7284e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score of these 1000 test data sentences is:  0.03251957333271974\n"
     ]
    }
   ],
   "source": [
    "average_bleu_scores=sum(bleu_scores_lst)/len(bleu_scores_lst)\n",
    "print(\"Average BLEU score of these 1000 test data sentences is: \",average_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdJlwYZdRpHL",
    "outputId": "c5f9630b-a71f-4d21-d056-8d9475f71cc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.46230595512422085,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.03589763288144409,\n",
       " 0.15218787864872976,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLVzLq8jX3j7"
   },
   "source": [
    "Character_Level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "b8apihPtV6he",
    "outputId": "8a4668d8-610e-4c67-ea7a-c4e37442bfc9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?\\n</td>\n",
       "      <td>Do you want me to reserve seat for you or not?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm thai. what do u do?\\n</td>\n",
       "      <td>I'm Thai. What do you do?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                             target\n",
       "0           0  ...   Do you want me to reserve seat for you or not?\\n\n",
       "1           1  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2           2  ...  They become more expensive already. Mine is li...\n",
       "3           3  ...                        I'm Thai. What do you do?\\n\n",
       "4           4  ...  Hi! How did your week go? Haven't heard from y...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('preprocessed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "tXs6gAuwV6js"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  x=x[:-1]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "K2ZI7m4WV6mn"
   },
   "outputs": [],
   "source": [
    "df['source']=df['source'].apply(preprocess)\n",
    "df['target']=df['target'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "vNi3h6NvV6o-",
    "outputId": "9adb47f4-885e-4dd0-cd0b-fcc75a8e2830"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>I'm Thai. What do you do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source                                             target\n",
       "0                    U wan me to \"chop\" seat 4 u nt?     Do you want me to reserve seat for you or not?\n",
       "1  Yup. U reaching. We order some durian pastry a...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?                          I'm Thai. What do you do?\n",
       "4  Hi! How did your week go? Haven heard from you...  Hi! How did your week go? Haven't heard from y..."
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['source','target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PScoGleBV6sq",
    "outputId": "cc5bb82a-b931-4f45-aa23-f824bcefff86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "kJ9NIzerUMyF"
   },
   "outputs": [],
   "source": [
    "def length(text):#for calculating the length of the sentence\n",
    "    return len(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "zCNdCYXYXBG3"
   },
   "outputs": [],
   "source": [
    "df=df[df['source'].apply(length)<=170]\n",
    "df=df[df['target'].apply(length)<=200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeXF3evYXBKD",
    "outputId": "b9fd66c5-36ed-4129-e6f2-4255fef052b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 2)"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "9l_rKDqJUMzo",
    "outputId": "7b3c4ceb-0f85-41a8-875b-f323b1474da4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>target_in</th>\n",
       "      <th>target_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?</td>\n",
       "      <td>\\tDo you want me to reserve seat for you or not?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "      <td>\\tYeap. You reaching? We ordered some Durian p...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "      <td>\\tThey become more expensive already. Mine is ...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>I'm Thai. What do you do?</td>\n",
       "      <td>\\tI'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "      <td>\\tHi! How did your week go? Haven't heard from...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  ...                                         target_out\n",
       "0                    U wan me to \"chop\" seat 4 u nt?  ...   Do you want me to reserve seat for you or not?\\n\n",
       "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?  ...                        I'm Thai. What do you do?\\n\n",
       "4  Hi! How did your week go? Haven heard from you...  ...  Hi! How did your week go? Haven't heard from y...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target_in'] = '\\t' + df['target'].astype(str)\n",
    "df['target_out'] = df['target'].astype(str) + '\\n'\n",
    "# only for the first sentance add a toke <end> so that we will have <end> in tokenizer\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "hcopkrX_UM1J"
   },
   "outputs": [],
   "source": [
    "df=df.drop('target',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "rhxX-xF2UM35",
    "outputId": "4dd23299-b3d9-4192-c4a3-e6a04b5f05ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target_in</th>\n",
       "      <th>target_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>\\tDo you want me to reserve seat for you or not?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>\\tYeap. You reaching? We ordered some Durian p...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>\\tThey become more expensive already. Mine is ...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>\\tI'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do?\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  ...                                         target_out\n",
       "0                    U wan me to \"chop\" seat 4 u nt?  ...   Do you want me to reserve seat for you or not?\\n\n",
       "1  Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2  They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
       "3                            I'm thai. what do u do?  ...                        I'm Thai. What do you do?\\n\n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "5NkUjDVOUM7e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(df, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNbMHZiDVmk8",
    "outputId": "86c1b843-60e4-4572-8e31-9bb4f441b299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, validation.shape)\n",
    "# for one sentence we will be adding <end> token so that the tokanizer learns the word <end>\n",
    "# with this we can use only one tokenizer for both encoder output and decoder output\n",
    "train.iloc[0]['target_in']= str(train.iloc[0]['target_in'])+'\\n'\n",
    "train.iloc[0]['target_out']= str(train.iloc[0]['target_out'])+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "NJ_LVN21TIo3"
   },
   "outputs": [],
   "source": [
    "tknizer_source = Tokenizer(filters=None,char_level=True,lower=False)\n",
    "tknizer_source.fit_on_texts(train['source'].values)\n",
    "tknizer_target = Tokenizer(filters=None,char_level=True,lower=False)\n",
    "tknizer_target.fit_on_texts(train['target_in'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVIMypotTIqx",
    "outputId": "42561487-c01f-4c7e-e269-61984df884fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "vocab_size_target=len(tknizer_target.word_index.keys())\n",
    "print(vocab_size_target)\n",
    "vocab_size_source=len(tknizer_source.word_index.keys())\n",
    "print(vocab_size_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVua0TLRTIsf",
    "outputId": "e5dd16ba-2787-44dc-b99c-b652765a0f03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 85)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknizer_target.word_index['\\t'], tknizer_target.word_index['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_aZK3PseTIuK"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        super().__init__()\n",
    "        self.vocab_size = inp_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_length = input_length\n",
    "        self.lstm_size= lstm_size\n",
    "        self.lstm_output=0\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.lstm = tf.keras.layers.LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "\n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer oupulstm_state_h,lstm_state_ct to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      \n",
    "      input_embedd                           = self.embedding(input_sequence)\n",
    "      lstm_state_h,lstm_state_c= states[0],states[1]\n",
    "      self.lstm_output,lstm_state_h,lstm_state_c=self.lstm(input_embedd)\n",
    "      return self.lstm_output,lstm_state_h,lstm_state_c\n",
    "\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      return [tf.zeros((batch_size,self.lstm_size)),tf.zeros((batch_size,self.lstm_size))]\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PZh5UkmWTI0f"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super().__init__()\n",
    "    self.scoring_function=scoring_function\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      pass\n",
    "    if self.scoring_function == 'general':\n",
    "      # Intialize variables needed for General score function here\n",
    "      self.weight=tf.keras.layers.Dense(att_units)\n",
    "    elif self.scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "      self.weight1=tf.keras.layers.Dense(att_units)\n",
    "      self.weight2=tf.keras.layers.Dense(att_units)\n",
    "      self.v=tf.keras.layers.Dense(1)\n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=2)\n",
    "        value=tf.matmul(encoder_output,decoder_hidden_state)\n",
    "    elif self.scoring_function == 'general':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=2)\n",
    "        value=tf.matmul(self.weight(encoder_output),decoder_hidden_state)\n",
    "    elif self.scoring_function == 'concat':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state=tf.expand_dims(decoder_hidden_state,axis=1)\n",
    "        value=self.v(tf.nn.tanh(self.weight1(decoder_hidden_state)+self.weight2(encoder_output)))\n",
    "    attention_weights=tf.nn.softmax(value,axis=1)\n",
    "    context_vector=attention_weights*encoder_output\n",
    "    return tf.reduce_sum(context_vector,axis=1),attention_weights\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ZqLu3nywHmyM"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super().__init__()\n",
    "      self.tar_vocab_size = tar_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length \n",
    "      self.dec_units = dec_units\n",
    "      self.score_fun = score_fun\n",
    "      self.att_units = att_units\n",
    "      # we are using embedding_matrix and not training the embedding layer\n",
    "      self.embedding = tf.keras.layers.Embedding(input_dim=self.tar_vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\", trainable=True)\n",
    "      self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True)\n",
    "      self.dense = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "      self.attention = Attention(self.score_fun,self.att_units)\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    output = self.embedding(input_to_decoder)\n",
    "    context_vector,attention_weights = self.attention(state_h,encoder_output)\n",
    "    context_vector1 = tf.expand_dims(context_vector,1)\n",
    "    concat = tf.concat([output,context_vector1],axis=-1)\n",
    "    decoder_output,state_h,state_c = self.lstm(concat,initial_state=[state_h,state_c])\n",
    "    final_output = self.dense(decoder_output)\n",
    "    final_output = tf.reshape(final_output,(-1,final_output.shape[2]))\n",
    "    return final_output,state_h,state_c,attention_weights,context_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "qjOxZZKPHmz1"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super(Decoder,self).__init__()\n",
    "      self.vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units=dec_units\n",
    "      self.att_units=att_units\n",
    "      self.score_fun=score_fun\n",
    "      self.onestepdecoder=One_Step_Decoder(self.vocab_size,self.embedding_dim,self.input_length,self.dec_units ,self.score_fun ,self.att_units)\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        all_outputs=tf.TensorArray(tf.float32,size=tf.shape(input_to_decoder)[1])\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        #Iterate till the length of the decoder input\n",
    "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output,state_h,state_c,attention_weights,context_vector=self.onestepdecoder(input_to_decoder[:,timestep:timestep+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
    "\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs=all_outputs.write(timestep,output)\n",
    "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        # Return the tensor array\n",
    "        return all_outputs\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "fp9nhmGsHm1q"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,encoder_inputs_length,decoder_inputs_length, output_vocab_size,batch_size,score_fun):\n",
    "    #Intialize objects from encoder decoder\n",
    "    super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "    self.batch_size=batch_size\n",
    "    self.encoder = Encoder(vocab_size_source+1,100,128,encoder_inputs_length)\n",
    "    self.decoder = Decoder(vocab_size_target+1,100,decoder_inputs_length,128,score_fun,128)\n",
    "    \n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    input,output = data[0], data[1]\n",
    "    initial_state=self.encoder.initialize_states(self.batch_size)\n",
    "    encoder_output, encoder_h, encoder_c = self.encoder(input,initial_state)\n",
    "    decoder_output= self.decoder(output, encoder_output, encoder_h, encoder_c)\n",
    "    return decoder_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ygx_9YW9Hm2-"
   },
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
    "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
    "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
    "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
    "    during preprocessing to make equal length for all the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Q33T-iHDUYQU"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, tknizer_source, tknizer_target, source_len,target_len):\n",
    "        self.encoder_inps = df['source'].values\n",
    "        self.decoder_inps = df['target_in'].values\n",
    "        self.decoder_outs = df['target_out'].values\n",
    "        self.tknizer_target = tknizer_target\n",
    "        self.tknizer_source = tknizer_source\n",
    "        self.source_len = source_len\n",
    "        self.target_len = target_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_source.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_target.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_target.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.source_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.target_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.target_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVCq83XIUd3l",
    "outputId": "b2d66313-de9a-437d-c88d-bbbec5dc55cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 170) (512, 200) (512, 200)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_source, tknizer_target,170,200)\n",
    "test_dataset  = Dataset(validation, tknizer_source, tknizer_target,170,200)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=20)\n",
    "\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xk0YayPGUkN7",
    "outputId": "ec9e6f55-38c4-4e40-ba90-2ba16a832c12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 16s 3s/step - loss: 1.5136 - val_loss: 1.5831\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.2368 - val_loss: 1.4793\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.1685 - val_loss: 1.4271\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.1407 - val_loss: 1.3892\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.1088 - val_loss: 1.3515\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.0753 - val_loss: 1.3076\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.0427 - val_loss: 1.2692\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.0122 - val_loss: 1.2346\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9857 - val_loss: 1.2067\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9659 - val_loss: 1.1834\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.9485 - val_loss: 1.1672\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9367 - val_loss: 1.1538\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9264 - val_loss: 1.1410\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9184 - val_loss: 1.1300\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9109 - val_loss: 1.1238\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.9045 - val_loss: 1.1198\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8997 - val_loss: 1.1138\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8954 - val_loss: 1.1088\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8921 - val_loss: 1.1055\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8892 - val_loss: 1.1024\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8868 - val_loss: 1.0994\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8845 - val_loss: 1.0968\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8825 - val_loss: 1.0947\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8809 - val_loss: 1.0930\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8795 - val_loss: 1.0911\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8781 - val_loss: 1.0899\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8770 - val_loss: 1.0895\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8757 - val_loss: 1.0883\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8747 - val_loss: 1.0868\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8736 - val_loss: 1.0864\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8726 - val_loss: 1.0852\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8717 - val_loss: 1.0840\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8709 - val_loss: 1.0836\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8698 - val_loss: 1.0823\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8691 - val_loss: 1.0828\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8682 - val_loss: 1.0818\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8674 - val_loss: 1.0811\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8664 - val_loss: 1.0801\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8656 - val_loss: 1.0800\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8651 - val_loss: 1.0794\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8656 - val_loss: 1.0777\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8660 - val_loss: 1.0786\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8641 - val_loss: 1.0774\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8622 - val_loss: 1.0762\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8605 - val_loss: 1.0769\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8602 - val_loss: 1.0758\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8595 - val_loss: 1.0756\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8585 - val_loss: 1.0755\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8583 - val_loss: 1.0746\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8571 - val_loss: 1.0738\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8560 - val_loss: 1.0742\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8548 - val_loss: 1.0746\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8540 - val_loss: 1.0726\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8528 - val_loss: 1.0733\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8522 - val_loss: 1.0739\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8515 - val_loss: 1.0744\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8505 - val_loss: 1.0754\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8502 - val_loss: 1.0737\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8486 - val_loss: 1.0742\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8474 - val_loss: 1.0754\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8461 - val_loss: 1.0720\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8448 - val_loss: 1.0728\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8435 - val_loss: 1.0744\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8442 - val_loss: 1.0754\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8426 - val_loss: 1.0732\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8413 - val_loss: 1.0717\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8397 - val_loss: 1.0706\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8387 - val_loss: 1.0734\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8371 - val_loss: 1.0734\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8358 - val_loss: 1.0698\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8349 - val_loss: 1.0705\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8332 - val_loss: 1.0714\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8314 - val_loss: 1.0683\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8296 - val_loss: 1.0681\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8276 - val_loss: 1.0679\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8263 - val_loss: 1.0668\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8245 - val_loss: 1.0675\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8228 - val_loss: 1.0682\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8209 - val_loss: 1.0658\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8190 - val_loss: 1.0655\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8178 - val_loss: 1.0660\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8153 - val_loss: 1.0671\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8160 - val_loss: 1.0688\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8132 - val_loss: 1.0664\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8102 - val_loss: 1.0654\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8081 - val_loss: 1.0653\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.8054 - val_loss: 1.0664\n",
      "Epoch 88/100\n",
      "1/3 [=========>....................] - ETA: 5s - loss: 0.8124"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-32a430cf4dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create an object of encoder_decoder Model class, \n",
    "# Compile the model and fit the model\n",
    "# Implement teacher forcing while training your model. You can do it two ways.\n",
    "# Prepare your data, encoder_input,decoder_input and decoder_output\n",
    "# if decoder input is \n",
    "# <start> Hi how are you\n",
    "# decoder output should be\n",
    "# Hi How are you <end>\n",
    "# i.e when you have send <start>-- decoder predicted Hi, 'Hi' decoder predicted 'How' .. e.t.c\n",
    "\n",
    "# or\n",
    " \n",
    "# model.fit([train_ita,train_eng],train_eng[:,1:]..)\n",
    "# Note: If you follow this approach some grader functions might return false and this is fine.\n",
    "model  = encoder_decoder(encoder_inputs_length=170,decoder_inputs_length=200,output_vocab_size=vocab_size_target,batch_size=512,score_fun=\"concat\")\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(optimizer=optimizer,loss=loss_function)\n",
    "train_steps=train.shape[0]//512\n",
    "valid_steps=validation.shape[0]//20\n",
    "model.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=100, validation_data=test_dataloader, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "uIsxBsjZW79j"
   },
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "units=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_JI8L5VjW8Bk"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "  E. Call plot_attention(#params)\n",
    "  F. Return the predicted sentence\n",
    "  '''\n",
    "  initial_state_enc=[np.zeros((batch_size,units)),np.zeros((batch_size,units))]\n",
    "  inp_seq = tknizer_source.texts_to_sequences([input_sentence])\n",
    "  inp_seq = pad_sequences(inp_seq,padding='post',maxlen=170)\n",
    "\n",
    "  en_outputs,state_h , state_c = model.layers[0](tf.constant(inp_seq),initial_state_enc)\n",
    "  cur_vec = tf.constant([[tknizer_target.word_index['\\t']]])\n",
    "  pred = []\n",
    "  #Here 20 is the max_length of the sequence\n",
    "  for i in range(200):\n",
    "    output,state_h,state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(cur_vec,en_outputs,state_h,state_c)\n",
    "    cur_vec = np.reshape(np.argmax(output), (1, 1))\n",
    "    pred.append(tknizer_target.index_word[cur_vec[0][0]])\n",
    "    if(pred[-1]=='\\n'):\n",
    "      break\n",
    "    translated_sentence = ' '.join(pred)\n",
    "  return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "6SYABmhxW8G0",
    "outputId": "826bfebf-09d5-47fe-d4b9-f052320b5666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi its me you are probably having too much fun to get this message but i thought id txt u cos im bored! and james has been farting at me all night\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n",
      "Yupz... Luv my trip...Weather is great too... Cant take e heat here now...\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n",
      "Thanx 4 the time we've spent 2geva, its bin mint! Ur my Baby and all I want is u!\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n",
      "Hey....I know its rude of me not to do something abt e fone.N i'm sorry it died on u.\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n",
      "Haha... Okay... Sure thing must carry around sch ah....\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n",
      "Yup... Wah. How come you choose comp sci? After discussion with parents and sis? What mod ü need to take? Ü need to take stats? And ü doing gem this sem ?\n",
      "H i m m m e e r e r   o r o u r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o u r e r   n o u r e r   n o u r o n o\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7433921a2b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-e2ceec33b5ae>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m#Here 20 is the max_length of the sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monestepdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mcur_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtknizer_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2487\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_layers\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   2821\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m     for m in self._flatten_modules(\n\u001b[0;32m-> 2823\u001b[0;31m         recursive=recursive, include_self=include_self):\n\u001b[0m\u001b[1;32m   2824\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_modules\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         \u001b[0;31m# Metrics are not considered part of the Layer's topology.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m         if (isinstance(trackable_obj, module.Module) and\n\u001b[0;32m-> 2854\u001b[0;31m             not isinstance(trackable_obj, metrics_mod.Metric)):\n\u001b[0m\u001b[1;32m   2855\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mtrackable_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2856\u001b[0m           \u001b[0;31m# Introspect recursively through sublayers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in validation['source']:\n",
    "  pred=predict(i)\n",
    "  print(i)\n",
    "  print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEb2rA9McJnU",
    "outputId": "c4c0b613-4eb2-4339-faff-94ad66fcf714"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "import nltk.translate.bleu_score as bleu\n",
    "bleu_scores_lst=[]\n",
    "for i in validation[:]['source']:\n",
    "  reference = [i.split(),] # the original\n",
    "  predicted=predict(i)\n",
    "  translation = predicted.split()\n",
    "  values=bleu.sentence_bleu(reference, translation)\n",
    "  bleu_scores_lst.append(values)\n",
    "\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAADuId4cPI_",
    "outputId": "d2071a23-4319-4022-f270-d0d5aa6c5c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score of these 1000 test data sentences is:  0.08111379872068883\n"
     ]
    }
   ],
   "source": [
    "average_bleu_scores=sum(bleu_scores_lst)/len(bleu_scores_lst)\n",
    "print(\"Average BLEU score of these 1000 test data sentences is: \",average_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JNnXh0VcUR5",
    "outputId": "236123ea-46c7-4312-ac7f-04d5fa02736c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.287190894500909,\n",
       " 0,\n",
       " 0.34152945510447685,\n",
       " 0,\n",
       " 0.287190894500909,\n",
       " 0.34152945510447685,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.287190894500909,\n",
       " 0,\n",
       " 0.287190894500909,\n",
       " 0,\n",
       " 0.3779644730092272,\n",
       " 0.287190894500909,\n",
       " 0,\n",
       " 0.287190894500909,\n",
       " 0]"
      ]
     },
     "execution_count": 208,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H7W6JIrerd4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Attention_using_Fasttext.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
